---
title: mit6.824-GFS-FAQ(翻译)
comments: true
typora-root-url: ../../source/
date: 2020-04-16 20:38:19
tags:
- 6.824
- GFS
---

# GFS相关问题

## 1. 为什么原子性的记录只保证最少一次，而不是精确的一次？

> ```
> Q: Why is atomic record append at-least-once, rather than exactly once?
> ```

3.1节（第7步）说，如果在其中一个写入失败二次，客户端重试写入。 那会导致数据要在不失败的副本中多次添加。 不同的设计可能会检测到重复的客户请求，尽管任意故障（例如，原始故障之间的主要故障请求和客户的重试）。 您将在实验3中实现这样的设计，在复杂性和性能上付出相当大的代价。

> ```
> Section 3.1, Step 7, says that if a write fails at one of the secondaries, the client re-tries the write. That will cause the data to be appended more than once at the non-failed replicas. A different design could probably detect duplicate client requests despite arbitrary failures (e.g. a primary failure between the original request and the client's retry). You'll implement such a design in Lab 3, at considerable expense in complexity and performance.
> ```

## 2. 应用程序如何知道块的哪些部分是padding，哪些是重复的记录

> ```
> Q: How does an application know what sections of a chunk consist of
> padding and duplicate records?
> ```

为了检测填充，应用程序在有效记录的开头写入可预测的Magic Number，或包含可能会出现的校验和仅当记录有效时才有效。 该应用程序可以检测通过在记录中包含唯一ID进行重复。 然后，如果它读取一个与先前的记录具有相同ID的记录，它知道它们彼此重复。 GFS为应用程序提供了一个库处理这些情况。

> ```
> To detect padding, applications can put a predictable magic number at the start of a valid record, or include a checksum that will likely only be valid if the record is valid. The application can detect duplicates by including unique IDs in records. Then, if it reads a
> record that has the same ID as an earlier record, it knows that they are duplicates of each other. GFS provides a library for applications that handles these cases.
> ```

## 3. 给定原子记录后，客户如何找到他们的数据，将其以不可预测的偏移量写入文件中？

> ```
> Q: How can clients find their data given that atomic record append writes it at an unpredictable offset in the file?
> ```

> ```
> A: Append (and GFS in general) is mostly intended for applications that sequentially read entire files. Such applications will scan the file looking for valid records (see the previous question), so they don't need to know the record locations in advance. For example, the file might contain the set of link URLs encountered by a set of concurrent web crawlers. The file offset of any given URL doesn't matter much; readers just want to be able to read the entire set of URLs.
> ```

## 4. 什么是校验和

> ```
> Q: What's a checksum?
> ```

校验和算法将一个字节块作为输入并返回一个单个数字是所有输入字节的函数。 例如，一个简单校验和可能是输入中所有字节的总和（mod一些大数字）。 GFS存储每个块的校验和以及块。 块服务器在其磁盘上写入块时，首先计算新块的校验和，并将校验和保存在磁盘上以及大块 当块服务器从磁盘读取块时，它还读取先前保存的校验和，从从磁盘读取块，并检查两个校验和是否匹配。 如果数据已被磁盘损坏，校验和不匹配，并且chunkserver将知道返回错误。 另外，一些GFS应用程序根据应用程序定义存储自己的校验和GFS文件中的记录，以区分正确的记录和填充。 CRC32是校验和算法的示例。

> ```
> A: A checksum algorithm takes a block of bytes as input and returns a single number that's a function of all the input bytes. For example, a simple checksum might be the sum of all the bytes in the input (mod some big number). GFS stores the checksum of each chunk as well as the chunk. When a chunkserver writes a chunk on its disk, it first computes the checksum of the new chunk, and saves the checksum on disk as well as the chunk. When a chunkserver reads a chunk from disk, it also reads the previously-saved checksum, re-computes a checksum from
> the chunk read from disk, and checks that the two checksums match. If the data was corrupted by the disk, the checksums won't match, and the chunkserver will know to return an error. Separately, some GFS applications stored their own checksums, over application-defined 
> records, inside GFS files, to distinguish between correct records and padding. CRC32 is an example of a checksum algorithm.
> ```

## 5. 论文中提到的引用计数是什么

> ```
> Q: The paper mentions reference counts -- what are they?
> ```

它们是快照写时复制实现的一部分。当GFS创建快照时，它不会复制数据块，而是复制增加每个块的参考计数器。 这使得创建一个快照价格便宜。 如果客户写了一个块，而主人写了注意引用计数大于一，高手在前制作副本，以便客户端可以更新副本（而不是快照的一部分）。 您可以将其视为延迟复制，直到绝对必要。 希望不是所有的块将被修改，可以避免制作一些副本。

> ```
> A: They are part of the implementation of copy-on-write for snapshots.When GFS creates a snapshot, it doesn't copy the chunks, but instead increases the reference counter of each chunk. This makes creating a snapshot inexpensive. If a client writes a chunk and the master
> notices the reference count is greater than one, the master first makes a copy so that the client can update the copy (instead of the chunk that is part of the snapshot). You can view this as delaying the copy until it is absolutely necessary. The hope is that not all chunks
> will be modified and one can avoid making some copies.
> ```

## 6. 如果一个程序使用了标准的POSIX文件操作接口，为了使用GFS，它是否需要进行修改

> ```
> Q: If an application uses the standard POSIX file APIs, would it need to be modified in order to use GFS?
> ```

是的，但是GFS不适用于现有应用程序。 它是设计用于新编写的应用程序，例如MapReduce程序。

> ```
> A: Yes, but GFS isn't intended for existing applications. It is designed for newly-written applications, such as MapReduce programs.
> ```

## 7. GFS如何确定最近副本的位置

> ```
> Q: How does GFS determine the location of the nearest replica?
> ```

本文暗示GFS会根据服务器的IP地址来执行此操作。存储可用副本的服务器。 在2003年，Google必须具备分配IP地址的方式，如果两个IP地址接近在IP地址空间中彼此连接，那么它们也彼此靠近在机房。

> ```
> A: The paper hints that GFS does this based on the IP addresses of the servers storing the available replicas. In 2003, Google must have assigned IP addresses in such a way that if two IP addresses are close to each other in IP address space, then they are also close together
> in the machine room.
> ```

## 8. 假设S1是块的Primary，并且master和S1之间的网络故障。 master会注意到这个问题并指定其他服务器作为Primary，比如S2。 由于S1实际上并没有失败，所以现在在同一个块中存在两个Primary？
> ```
> Q: Suppose S1 is the primary for a chunk, and the network between the master and S1 fails. The master will notice and designate some other server as primary, say S2. Since S1 didn't actually fail, are there now two primaries for the same chunk?
> ```

那将是一场灾难，因为两个Primary都可能适用对同一块的不同更新。 幸运的是，GFS的租赁机制防止这种情况。 主机授予S1 60秒的租约。 当租约到期时，S1知道不再是主要服务器。 master不会将租约授予S2，直到之前的租约授予S1
到期。 因此，直到S1停止后，S2才开始充当主要对象。

> ```
> A: That would be a disaster, since both primaries might apply different updates to the same chunk. Luckily GFS's lease mechanism prevents this scenario. The master granted S1 a 60-second lease to be primary. S1 knows to stop being primary when its lease expires. The master won't grant a lease to S2 until the previous lease to S1 expires. So S2 won't start acting as primary until after S1 stops.
> ```

## 9. 64MB对于块大小来说是否太大了

> ```
> Q: 64 megabytes sounds awkwardly large for the chunk size!
> ```

64 MB的块大小是master中book-keeping的单位，以及块服务器分片文件的粒度。 客户端可以发出更小的读取和写入-他们没有被迫处理总共64 MB块。 使用这么大的块大小是为了减少master数据库中元数据表的大小，并避免限制想要进行大量传输以减少开销的客户端。 另一方面，小于64 MB的文件不会得到太多并行性。

> ```
> A: The 64 MB chunk size is the unit of book-keeping in the master, and the granularity at which files are sharded over chunkservers. Clients could issue smaller reads and writes -- they were not forced to deal in whole 64 MB chunks. The point of using such a big chunk size is to reduce the size of the meta-data tables in the master, and to avoid limiting clients that want to do huge transfers to reduce overhead. On the other hand, files less than 64 MB in size do not get much parallelism.
> ```

## 10. Google现在还在用GFS吗

> ```
> Q: Does Google still use GFS?
> ```

> ```
> A: Rumor has it that GFS has been replaced by something called Colossus, with the same overall goals, but improvements in master performance and fault-tolerance.
> ```

## 11. How acceptable is it that GFS trades correctness for performance and simplicity?

> ```
> Q: How acceptable is it that GFS trades correctness for performance and simplicity?
> ```

> ```
> A: This a recurring theme in distributed systems. Strong consistency usually requires protocols that are complex and require chit-chat between machines (as we will see in the next few lectures). By exploiting ways that specific application classes can tolerate relaxed
> consistency, one can design systems that have good performance and sufficient consistency. For example, GFS optimizes for MapReduce applications, which need high read performance for large files and are OK with having holes in files, records showing up several times, and inconsistent reads. On the other hand, GFS would not be good for storing account balances at a bank.
> ```

## 12. master 失败了会发生什么

> ```
> Q: What if the master fails?
> ```

存在具有完整master状态的副本，论文的设计中在master fail后需要进行人工干预才能切换到master副本，稍后我们将看到如何使用Raft自动切换。

> ```
> A: There are replica masters with a full copy of the master state; the paper's design requires human intervention to switch to one of the replicas after a master failure (Section 5.1.3). We will see later how to build replicated services with automatic cut-over to a backup, using Raft.
> ```

## 13. single master是一个好的主意吗

> ```
> Q: Did having a single master turn out to be a good idea?
> ```

这个想法简化了初始化部署，但从长远来看并不是很好。 [本文](https://queue.acm.org/detail.cfm?id=1594206)说，随着时间的流逝和GFS使用的增加，有些事情出了问题。 文件数量增长到足以将所有文件的元数据存储在单个主机的RAM中是不合理的。 客户端数量增长到足以使单个主服务器没有足够的CPU能力为他们服务。 从发生故障的主服务器切换到其备份之一需要人工干预，这使恢复速度变慢。 显然，谷歌（Google）取代了GFS，即Colossus，将主服务器拆分为多个服务器，并具有更自动的主服务器故障恢复功能。

> ```
> A: That idea simplified initial deployment but was not so great in the long run. This article -- https://queue.acm.org/detail.cfm?id=1594206 -- 
> says that as the years went by and GFS use grew, a few things went wrong. The number of files grew enough that it wasn't reasonable to store all files' metadata in the RAM of a single master. The number of clients grew enough that a single master didn't have enough CPU power
> to serve them. The fact that switching from a failed master to one of its backups required human intervention made recovery slow. Apparently Google's replacement for GFS, Colossus, splits the master over multiple servers, and has more automated master failure recovery.
> ```